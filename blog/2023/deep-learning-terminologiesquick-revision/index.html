<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3>Deep Learning Terminologies‚Ää‚Äî‚ÄäQuick¬†Revision</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/788/0*awGgqaLH0A84J_Di"><figcaption>Photo by <a href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Kelly Sikkema</a> on¬†<a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p>Welcome to a comprehensive exploration of the fascinating world of deep learning terminologies. In the rapidly evolving landscape of artificial intelligence, understanding these fundamental concepts is like unlocking the keys to a treasure chest of knowledge. Whether you‚Äôre a seasoned data scientist, a budding AI enthusiast, or simply curious about the technology shaping our future, this comprehensive guide will provide you with a quick and clear revision of the essential terms you need to know. So, let‚Äôs embark on this learning adventure and unravel the mysteries of deep learning together.</p> <p>Happy revision! üîîüòä</p> <h3>Foundation &amp;¬†Concepts</h3> <ol> <li> <strong>Artificial Intelligence (AI):</strong> The simulation of human intelligence processes by machines, especially computer systems. It encompasses a variety of tasks including decision-making, visual perception, and speech recognition.</li> <li> <strong>Machine Learning:</strong> A subset of artificial intelligence that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions based on¬†data.</li> <li> <strong>Deep Learning:</strong> A category of machine learning that utilizes artificial neural networks with many layers (deep networks) to automatically learn and extract hierarchical features from¬†data.</li> <li> <strong>GeoAI (Geospatial Artificial Intelligence): </strong>A subset of AI that integrates geospatial data, enabling machines to derive insights from spatial relationships, topographies, and environmental dynamics.</li> <li> <strong>Natural Language Processing (NLP): </strong>A field of AI focused on the interaction between computers and human language, including tasks like language translation and sentiment analysis.</li> <li> <strong>Computer Vision: </strong>A field of artificial intelligence that focuses on enabling machines to interpret and understand visual information from the world, including images and¬†videos.</li> <li> <strong>Neurons (Nodes):</strong> The fundamental units in artificial neural networks that receive inputs, apply weights and activation functions and produce outputs to transmit information.</li> <li> <strong>Layer:</strong> A collection of neurons grouped together. Neural networks typically consist of input, hidden, and output¬†layers.</li> <li> <strong>Hidden Layer: </strong>A ‚Äúhidden layer‚Äù in the context of neural networks refers to one or more layers of artificial neurons (also known as nodes or units) that come between the input layer and the output layer. These hidden layers are a crucial component of feedforward neural networks, including multi-layer perceptrons (MLPs) and deep neural networks¬†(DNNs).</li> <li> <strong>Depth:</strong> The number of hidden layers in a neural network determines its depth. Networks with more hidden layers are referred to as ‚Äúdeep‚Äù networks. Deep neural networks are capable of learning hierarchical features and are often used in complex tasks like image recognition and natural language processing.</li> <li> <strong>Artificial Neural Networks (ANNs):</strong> Computational models inspired by the human brain, consisting of interconnected nodes (neurons) organized in layers to process and learn from¬†data.</li> <li> <strong>Multi-Layer Perceptron (MLP):</strong> A type of artificial neural network with multiple layers of interconnected neurons, commonly used in various machine learning¬†tasks.</li> <li> <strong>Convolution:</strong> A mathematical operation in CNNs that merges two sets of information. It‚Äôs used to apply filters to input data to extract essential features.</li> <li> <strong>CNN (Convolutional Neural Network):</strong> A deep learning algorithm that‚Äôs primarily used for analyzing visual imagery. They use convolutional layers to process and filter¬†inputs.</li> <li> <strong>Feedforward Neural Network (FNN):</strong> A type of neural network where information flows in one direction, from input to output, without loops or¬†cycles.</li> <li> <strong>Recurrent Neural Network (RNN):</strong> A type of neural network architecture that allows for connections that form cycles, making them suitable for sequential data, such as time series and natural language processing.</li> <li> <strong>Reinforcement Learning: </strong>A machine learning paradigm where agents learn to make decisions by interacting with an environment and receiving rewards.</li> <li> <strong>Supervised Learning: </strong>A machine learning technique where the model learns from labeled data, making predictions or classifications based on input features.</li> <li> <strong>Unsupervised Learning:</strong> A machine learning technique where the model learns from unlabeled data, discovering patterns and structures within the¬†data.</li> <li> <strong>Semi-Supervised Learning:</strong> A combination of supervised and unsupervised learning, where a model is trained on a mix of labeled and unlabeled data.</li> <li> <strong>Bias:</strong> An additional term in linear regression and neural networks that allows models to fit data better by introducing an offset. In the context of machine learning, ‚Äúbias‚Äù refers to the error introduced by approximating a real-world problem with a simplified model. It is one of the sources of error in machine learning algorithms, along with variance and irreducible error. Bias occurs when a model‚Äôs predictions consistently deviate from the actual values it is trying to¬†predict.</li> <li> <strong>Variance: </strong>The variability in model predictions across different datasets or samples. High variance can indicate overfitting.</li> <li> <strong>Underfitting (High Bias):</strong> Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data. It typically results in a model that performs poorly on both the training data and unseen data (test data). Underfit models have high bias because they make strong assumptions that don‚Äôt align with the true relationships in the¬†data.</li> <li> <strong>Overfitting (High Variance):</strong> Overfitting, on the other hand, occurs when a machine learning model is excessively complex and fits the training data too closely, capturing noise and random fluctuations. While overfit models perform well on the training data, they generalize poorly to unseen data, as they essentially memorize the training examples rather than learning meaningful patterns. Overfit models have high variance because they are sensitive to small variations in the training¬†data.</li> <li> <strong>Bias-Variance Decomposition:</strong> A framework for understanding a model‚Äôs error as the sum of bias, variance, and irreducible error components.</li> <li> <strong>Bias and Variance Trade-off:</strong> A fundamental concept in model training where reducing bias often increases variance and vice versa. Striking the right balance is crucial for model performance.</li> <li> <strong>Regularization: </strong>Techniques used to prevent overfitting by adding a penalty to the loss function.</li> </ol> <h3>Data &amp; Preprocessing</h3> <ol> <li> <strong>Numerical Data: </strong>Data that consists of numbers and can be represented as continuous or discrete¬†values.</li> <li> <strong>Categorical Data: </strong>Data that represents categories or labels and is often non-numeric. It can be nominal (unordered) or ordinal (ordered).</li> <li> <strong>Text Data:</strong> Unstructured data consisting of text, often requiring natural language processing (NLP) techniques for analysis.</li> <li> <strong>Time Series Data:</strong> Data that is collected or recorded over a sequence of time intervals, making it suitable for time-based analysis and forecasting.</li> <li> <strong>Tensors: </strong>Tensors are multi-dimensional arrays that can hold data of various types, including numbers, vectors, matrices, and higher-dimensional data. They generalize scalars, vectors, and matrices. Scalars are 0th-order tensors, vectors are 1st-order tensors, matrices are 2nd-order tensors, and higher-dimensional arrays are higher-order tensors. They are the primary data structures used in deep learning frameworks like TensorFlow and PyTorch. They store data such as input features, weights, biases, and activation outputs in neural networks.</li> <li> <strong>Arrays: </strong>Arrays are multi-dimensional data structures used in programming languages and libraries for efficient data storage and manipulation. They are typically homogeneous, meaning they store data of the same data type (e.g., all integers, all floating-point numbers).</li> <li> <strong>Annotations:</strong> Descriptive labels on data, commonly used in supervised learning. For images, these can be bounding boxes, segmentation masks, or¬†points.</li> <li> <strong>Data Leakage: </strong>This occurs when information outside the training data set is used inappropriately to create the model, leading to overly optimistic performance estimates.</li> <li> <strong>Data Imbalance: </strong>A common problem in classification tasks is when one class has significantly fewer samples than¬†others.</li> <li> <strong>Data Augmentation:</strong> Techniques that introduce random modifications or variations to training data to increase its diversity and improve model generalization.</li> <li> <strong>Image Augmentation:</strong> Techniques specific to image data, such as rotation, flipping, and adding noise, to generate new training examples from existing¬†images.</li> <li> <strong>Data Cleaning: </strong>The process of identifying and correcting or removing errors and inconsistencies in datasets. This may involve handling missing values, outliers, and duplicate entries.</li> <li> <strong>Data Transformation:</strong> The process of converting data from one format or structure to another. Examples include scaling features, encoding categorical variables, and creating derived features.</li> <li> <strong>Feature Engineering:</strong> The process of creating new features from existing data to improve model performance. Feature engineering involves selecting, modifying, or combining features to make them more informative.</li> <li> <strong>Normalization: </strong>Scaling features to a standard range, often between 0 and 1, to ensure that they have similar scales and do not dominate the learning¬†process.</li> <li> <strong>Standardization:</strong> Transforming features to have a mean of 0 and a standard deviation of 1. It is especially useful for algorithms that assume a normal distribution of¬†data.</li> <li> <strong>Imputation: </strong>The process of filling in missing values in a dataset with estimated or imputed values. Common imputation techniques include mean imputation, median imputation, or using predictive models to estimate missing¬†values.</li> <li> <strong>Outlier Detection:</strong> Identifying and handling data points that significantly deviate from the majority of the data. Outliers can be detected using statistical methods or machine learning algorithms.</li> <li> <strong>Feature Scaling:</strong> Rescaling features to ensure that they have similar scales. Common techniques include min-max scaling and z-score¬†scaling.</li> <li> <strong>One-Hot Encoding:</strong> A technique for representing categorical data as binary vectors, with one ‚Äò1‚Äô for the category and ‚Äò0‚Äôs for all¬†others.</li> <li> <strong>Label Encoding: </strong>Converting categorical variables into numerical values by assigning each category a unique¬†integer.</li> <li> <strong>Feature Selection:</strong> The process of selecting a subset of the most relevant features from the dataset while discarding less important or redundant features. This can improve model efficiency and reduce overfitting.</li> <li> <strong>Principal Component Analysis (PCA):</strong> A dimensionality reduction technique that finds the linear combinations of features that capture the most variance in the¬†data.</li> <li> <strong>Large Scale:</strong> Refers to extensive datasets or models. In deep learning, it often refers to training on a massive amount of data or using large¬†models.</li> <li> <strong>Geospatial Data:</strong> Information about objects, events, or phenomena that have a location on the surface of the Earth. This could be represented as coordinates, addresses, or polygons.</li> <li> <strong>RGB (Red, Green, Blue): </strong>Standard color model used in¬†imaging.</li> <li> <strong>Data Loader:</strong> In PyTorch, it provides an iterator over a dataset, making it easier to batch and shuffle the¬†data.</li> <li> <strong>Random Sampling: </strong>The process of selecting data points from a dataset randomly. It is often used to create training and testing subsets from a larger¬†dataset.</li> <li> <strong>Stratified Sampling:</strong> A sampling method that maintains the same proportion of categories or classes in the sample as in the original dataset. It is useful for imbalanced datasets.</li> <li> <strong>Train-Test Split:</strong> Dividing the dataset into two subsets: one for training the model and one for evaluating its performance (testing).</li> <li> <strong>Cross-Validation:</strong> A technique where the dataset is divided into multiple subsets (folds), and the model is trained and evaluated multiple times, ensuring robust performance assessment.</li> </ol> <h3>Neural Network Components</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*aQ_f5d5UBVaQ75qc"><figcaption>Photo by <a href="https://unsplash.com/@averey?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Robina Weermeijer</a> on¬†<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <ol> <li> <strong>Neural Network:</strong> Network of neurons¬†(nodes).</li> <li> <strong>Convolutional Layer:</strong> A layer in a CNN that applies convolutional operations to extract features from input data. Commonly used in image processing.</li> <li> <strong>Filter (Kernel):</strong> A small matrix used in the convolution operation to extract features from input¬†data.</li> <li> <strong>Pooling Layer:</strong> A layer in a CNN that performs down-sampling by reducing the dimensionality of feature maps, typically using max-pooling or average-pooling.</li> <li> <strong>Stride: </strong>A hyperparameter in CNNs that defines the step size of the filter during convolution.</li> <li> <strong>Padding:</strong> Adding extra pixels around the input data to control the spatial dimensions of feature maps during convolution.</li> <li> <strong>Feature Map:</strong> The output of a convolutional layer, representing the presence of certain features in the input¬†data.</li> <li> <strong>CNN Architectures:</strong> Well-known CNN architectures include LeNet, AlexNet, VGGNet, GoogLeNet, and ResNet, each with specific layer configurations and performance characteristics.</li> <li> <strong>Model Training:</strong> The process of feeding data to a machine learning or deep learning model and adjusting the model‚Äôs weights based on the output of the loss function.</li> <li> <strong>Evaluation:</strong> The process of testing the model‚Äôs performance on a separate set of data to gauge its accuracy and reliability.</li> <li> <strong>Model Selection: </strong>Experiment with different machine learning algorithms and model architectures to find the one that best fits your data while avoiding underfitting or overfitting.</li> <li> <strong>Feature Engineering:</strong> Carefully select and preprocess features to ensure that relevant information is provided to the¬†model.</li> <li> <strong>IOU (Intersection over Union):</strong> A metric in object detection that measures the overlap between two boundaries.</li> <li> <strong>Activation Functions:</strong> Functions that introduce non-linearity into the network, allowing it to learn from the error and adjust, essential for learning complex patterns. Common activation functions include ReLU, Tanh, Leaky ReLU, Parametric ReLU (PReLU), and¬†Swish.</li> <li> <strong>ReLU (Rectified Linear Activation):</strong> An activation function that replaces all negative pixel values in the feature map with¬†zero.</li> <li> <strong>Batch:</strong> A subset of the dataset used during the training of neural networks. Batches are processed sequentially.</li> <li> <strong>Epoch:</strong> A single pass through the entire training dataset during training. Multiple epochs are typically required to train a model effectively.</li> <li> <strong>Optimizer:</strong> Algorithms or methods used to adjust the attributes of the neural network, such as weights and learning rate, to reduce¬†losses.</li> <li> <strong>Scheduler:</strong> In the context of training deep learning models, it adjusts the learning rate over¬†time.</li> <li> <strong>Early Stopping:</strong> A regularization method that stops the training process once a specified metric stops improving.</li> <li> <strong>Feature Maps:</strong> The output of each convolutional layer, highlighting the learned features from the input¬†data.</li> <li> <strong>Backpropagation:</strong> A supervised learning algorithm used for training neural networks. It adjusts the weights of neurons by calculating the gradient of the loss function.</li> <li> <strong>Forward Pass:</strong> The initial phase in training where input data is passed through the network to get the predicted output.</li> <li> <strong>Dropout:</strong> A regularization technique where randomly selected neurons are ignored during training, helping to prevent overfitting.</li> <li> <strong>Pooling:</strong> A down-sampling operation that reduces the dimensionality of the feature map, making the computations faster and more robust to variations.</li> <li> <strong>Fine-tuning:</strong> The process of tweaking a pre-trained model for a specific¬†task.</li> <li> <strong>Learning Rate:</strong> A hyper-parameter that determines the step size at each iteration while moving towards a minimum in the loss function.</li> <li> <strong>Momentum:</strong> A term used in optimization that helps accelerate gradient vectors in the right direction.</li> <li> <strong>Hyperparameter:</strong> Parameters in the model that are set before training and determine the training process‚Äôs overall behavior.</li> <li> <strong>Hyperparameter Tuning:</strong> Adjust hyperparameters like learning rate, regularization strength, and model complexity to find the right¬†balance.</li> <li> <strong>Weight Initialization:</strong> Weight initialization is a critical step in training neural networks. It involves setting the initial values of the weights in the network before the training process begins. Proper weight initialization can significantly impact the training process, convergence speed, and overall performance of the neural¬†network.</li> <li> <strong>He Initialization: </strong>Named after Kaiming He, another prominent researcher, He initialization is designed for ReLU (Rectified Linear Unit) activation functions. It initializes weights using a Gaussian distribution with mean 0 and a variance that depends on the number of input units. This initialization is effective in preventing the vanishing gradient problem when using ReLU activations, which can cause issues with convergence in deep networks.</li> <li> <strong>Batch Normalization: </strong>A normalization technique is applied to the inputs of each layer to accelerate training and improve convergence. Batch normalization normalizes the inputs of a layer by subtracting the batch mean and dividing by the batch standard deviation. This centers the activations around zero and scales them to have a standard deviation of¬†one.</li> <li> <strong>Mini-Batch Statistics: </strong>Batch normalization calculates the mean and standard deviation for each mini-batch during training. During inference, it uses the statistics of the entire training dataset, providing consistent results.</li> <li> <strong>Gradient Descent:</strong> An optimization technique used to minimize the loss function by iteratively adjusting model parameters in the direction of the steepest¬†descent.</li> <li> <strong>Gradient Descent Variants:</strong> Examples include Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad, which are different optimization algorithms.</li> <li> <strong>Mini-Batch Gradient Descent:</strong> A gradient descent optimization algorithm where the dataset is divided into small, manageable batches for training.</li> <li> <strong>Dropout Rate:</strong> The probability of neurons dropping out during dropout regularization.</li> <li> <strong>Weight Decay (L2 Regularization):</strong> A regularization technique that penalizes large weights in the neural network to prevent overfitting.</li> <li> <strong>Learning Rate Schedule:</strong> A strategy that adjusts the learning rate during training to improve convergence.</li> <li> <strong>Vanishing Gradient Problem:</strong> An issue in deep learning where gradients in the network become extremely small during training, hindering learning in deep networks.</li> <li> <strong>Exploding Gradient Problem:</strong> The opposite of vanishing gradients, where gradients become very large and lead to unstable training.</li> <li> <strong>Loss Function: </strong>This represents how far off our predictions are from the actual values. The aim of training is to minimize¬†this.</li> <li> <strong>Cost Function:</strong> A function that measures the difference between predicted values and actual values, used to guide the training¬†process.</li> <li> <strong>Softmax:</strong> An activation function that converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the¬†vector.</li> <li> <strong>Sigmoid:</strong> An activation function that maps any input value to a value between 0 and 1, often used for binary classification problems.</li> <li> <strong>Training Loss and Validation Loss:</strong> Represent the model‚Äôs error during the training phase and validation phase, respectively.</li> <li> <strong>Policy Gradient:</strong> A reinforcement learning technique where the model learns a policy that directly maps observations to actions, used in continuous action¬†spaces.</li> <li> <strong>Transfer Learning: </strong>A technique where a pre-trained model on a large dataset is used, and it‚Äôs further fine-tuned for a specific¬†task.</li> <li> <strong>Backbone:</strong> Refers to the architecture of a neural network, which extracts features from the input data. Common backbones include ResNet50 and ResNet101.</li> <li> <strong>Feature Classifier:</strong> The part of the model that takes the features from the backbone and determines the output, whether it‚Äôs a classification or regression task.</li> <li> <strong>FPN (Feature Pyramid Network):</strong> A feature extractor built for multi-scale object detection, enhancing the speed and accuracy of predictions.</li> <li> <strong>Fully Connected Layer (FC Layer):</strong> A traditional neural network layer where each neuron is connected to every neuron in the previous and subsequent layers. Typically used in the final layers of CNNs for classification tasks.</li> <li> <strong>ROI Align:</strong> A technique ensuring that the spatial variance is maintained when extracting exact feature maps for object detection.</li> <li> <strong>Distributed Computing:</strong> The use of multiple GPUs or computing nodes to train large deep learning models¬†faster.</li> <li> <strong>Inference: </strong>The process of using a trained model to make predictions on new, unseen¬†data.</li> <li> <strong>Activation Map:</strong> The output of a single neuron or unit in a neural network, highlighting its response to specific features.</li> <li> <strong>Confusion Matrix: </strong>A table used to evaluate the performance of a classification algorithm, showing true positives, true negatives, false positives, and false negatives.</li> <li> <strong>TPR (True Positive Rate):</strong> Also known as recall or sensitivity, TPR is the ratio of true positive predictions to the total actual positive cases. It measures the model‚Äôs ability to correctly identify positive instances.</li> <li> <strong>FPR (False Positive Rate): </strong>FPR is the ratio of false positive predictions to the total actual negative cases. It measures the rate of false alarms or incorrect positive predictions.</li> <li> <strong>TNR (True Negative Rate):</strong> Also known as specificity, TNR is the ratio of true negative predictions to the total actual negative¬†cases.</li> <li> <strong>FNR (False Negative Rate):</strong> FNR is the ratio of false negative predictions to the total actual positive cases. It measures the instances where the model failed to identify positive¬†cases.</li> <li> <strong>F1 Score:</strong> The F1 score is a metric that combines precision and recall, providing a balanced measure of a model‚Äôs performance. It is the harmonic mean of precision and recall and is particularly useful when dealing with imbalanced datasets.</li> <li> <strong>Precision and Recall:</strong> Metrics used to evaluate the performance of classification models, especially in imbalanced datasets.</li> <li> <strong>Precision:</strong> Precision is the ratio of true positive predictions to the total positive predictions made by the model. It measures the accuracy of positive predictions.</li> <li> <strong>Recall: </strong>Recall, also known as sensitivity or TPR, is the ratio of true positive predictions to the total actual positive cases. It measures the model‚Äôs ability to capture all positive instances.</li> <li> <strong>AP50 (Average Precision at 50% overlap):</strong> In object detection and image segmentation, AP50 is a common evaluation metric that measures the average precision of object detection when considering a 50% overlap threshold between predicted and ground truth bounding boxes or¬†masks.</li> <li> <strong>Jaccard Index: </strong>Also known as the Intersection over Union (IoU), the Jaccard Index is used in segmentation tasks to measure the similarity between the predicted and ground truth regions by calculating the intersection divided by the union of the two¬†regions.</li> <li> <strong>BCE (Binary Cross-Entropy):</strong> BCE is a loss function commonly used in binary classification problems. It measures the dissimilarity between predicted probabilities and actual binary¬†labels.</li> <li> <strong>CE (Cross-Entropy):</strong> Cross-Entropy is a loss function used in multi-class classification problems. It quantifies the difference between predicted class probabilities and actual class¬†labels.</li> <li> <strong>MAE (Mean Absolute Error): </strong>MAE is a metric used in regression tasks to measure the average absolute difference between predicted and actual values. It provides insight into the model‚Äôs prediction accuracy.</li> <li> <strong>RMSE (Root Mean Square Error): </strong>RMSE is another regression metric that measures the square root of the average of squared differences between predicted and actual values. It penalizes larger errors more¬†heavily.</li> <li> <strong>ROC Curve and AUC: </strong>Tools for assessing the performance of binary classification models by plotting the trade-off between true positive rate and false positive rate. While ROC and AUC are most commonly associated with binary classification, they can be adapted and applied to multi-class classification problems using various strategies. The choice of approach depends on the specific requirements and goals of the classification task, as well as the importance of individual classes.</li> </ol> <h3>Neural Network Architectures</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/788/0*zKo3jH28hUCYv5V3"><figcaption>Photo by <a href="https://unsplash.com/@alinnnaaaa?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Alina Grubnyak</a> on¬†<a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <ol> <li> <strong>Convolutional Neural Network (CNN):</strong> A deep learning algorithm that‚Äôs primarily used for analyzing visual imagery. They use convolutional layers to process and filter¬†inputs.</li> <li> <strong>LeNet:</strong> One of the earliest CNN architectures designed for handwritten digit recognition, consisting of convolutional and pooling¬†layers.</li> <li> <strong>AlexNet:</strong> A CNN architecture that gained prominence by winning the ImageNet Large Scale Visual Recognition Challenge in 2012, utilizing deep convolutional layers.</li> <li> <strong>VGGNet:</strong> A deep CNN architecture known for its simplicity and use of small 3x3 convolutional filters stacked on top of each¬†other.</li> <li> <strong>GoogLeNet (Inception):</strong> A CNN architecture that introduced the concept of inception modules with multiple filter sizes in parallel to capture features at different scales.</li> <li> <strong>Inception Modules: </strong>Inception modules are the hallmark of architecture. These modules are composed of multiple convolutional filters of different sizes (e.g., 1x1, 3x3, 5x5), which are applied in parallel to the input data. The outputs of these filters are then concatenated or stacked together. This allows the network to capture features at various scales and resolutions simultaneously, improving its ability to recognize complex patterns and¬†objects.</li> <li> <strong>1x1 Convolutions:</strong> In addition to traditional 3x3 and 5x5 convolutional filters, Inception makes extensive use of 1x1 convolutions. These 1x1 convolutions are primarily used to reduce the dimensionality of feature maps, reducing the computational cost. They also introduce non-linearity through activation functions.</li> <li> <strong>Xception:</strong> Xception (Extreme Inception) is an architecture inspired by Inception modules but takes the idea to an extreme by using depth-wise separable convolutions extensively. It aims for both high accuracy and efficiency.</li> <li> <strong>ResNet (Residual Network): </strong>A groundbreaking CNN architecture that uses residual connections to enable the training of very deep networks. Variants include ResNet50 and ResNet101.</li> <li> <strong>Residual Blocks: </strong>The core innovation of ResNet is the use of residual blocks. A residual block contains a shortcut connection (also known as a skip connection or identity shortcut) that bypasses one or more convolutional layers. Instead of trying to learn the desired output directly, residual blocks aim to learn the residual (the difference) between the desired output and the input. The shortcut connection allows gradients to flow directly through the network during training, mitigating the vanishing gradient¬†problem.</li> <li> <strong>Identity Mapping:</strong> In a residual block, if the input and output dimensions are the same, the shortcut connection simply adds the input to the output. This is known as identity mapping and is the default behavior when the dimensions match. If the dimensions differ, the shortcut connection may include a 1x1 convolutional layer to ensure compatibility.</li> <li> <strong>Deep Stacking:</strong> ResNet architectures are characterized by their extreme depth. Very deep networks with hundreds of layers can be trained effectively without experiencing vanishing gradients. ResNets come in various depths, such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152, each with a different number of¬†layers.</li> <li> <strong>DenseNet:</strong> A CNN architecture that encourages feature reuse by densely connecting each layer to every other layer in a feed-forward fashion. DenseNet architectures include DenseNet-121, DenseNet-169, and DenseNet-201.</li> <li> <strong>MobileNet:</strong> A family of CNN architectures designed for mobile and embedded devices, optimizing for efficiency and speed. Variants include MobileNetV1, MobileNetV2, and MobileNetV3.</li> <li> <strong>HRNet (High-Resolution Network):</strong> HRNet is designed to handle high-resolution images effectively and maintain detailed spatial information. It employs a high-resolution feature pyramid¬†network.</li> <li> <strong>Recurrent Neural Network (RNN): </strong>A type of neural network designed for sequential data, such as time series or natural language, by maintaining hidden¬†states.</li> <li> <strong>LSTM (Long Short-Term Memory): </strong>A specialized RNN architecture capable of learning and remembering long-term dependencies in sequences.</li> <li> <strong>GRU (Gated Recurrent Unit):</strong> Another type of RNN architecture, like LSTM, but with a simpler structure that often performs similarly.</li> <li> <strong>Bidirectional RNN:</strong> An RNN variant that processes input data in both forward and backward directions to capture dependencies from past and future time¬†steps.</li> <li> <strong>Sequence-to-Sequence (Seq2Seq):</strong> An architecture designed for tasks where the input and output are both sequences, such as machine translation and text generation.</li> <li> <strong>Attention Mechanism: </strong>An essential component in transformers, allowing models to focus on specific parts of the input sequence.</li> <li> <strong>Attention: </strong>A mechanism that allows neural networks to focus on specific parts of input data when making predictions, commonly used in natural language processing tasks.</li> <li> <strong>Transformer:</strong> A deep learning architecture that uses attention mechanisms to process sequences of data efficiently, enabling significant advancements in NLP, famous for models like BERT and¬†GPT.</li> <li> <strong>Autoencoder:</strong> A neural network architecture used for unsupervised learning and dimensionality reduction. It consists of an encoder and a¬†decoder.</li> <li> <strong>Encoder:</strong> The part of an autoencoder that maps input data to a lower-dimensional representation (encoding).</li> <li> <strong>Decoder:</strong> The part of an autoencoder that reconstructs the input data from the encoding.</li> <li> <strong>GAN (Generative Adversarial Network):</strong> A deep learning architecture consisting of a generator and a discriminator network. GANs are used for generating new data samples, such as images and¬†text.</li> <li> <strong>Generator:</strong> The component of a GAN that generates fake data¬†samples.</li> <li> <strong>Discriminator:</strong> The component of a GAN that evaluates whether a data sample is real (from the training dataset) or fake (generated by the generator).</li> <li> <strong>Pre-trained Model:</strong> A neural network model that has been trained on a large dataset for a specific task, often used as a starting point for transfer learning.</li> </ol> <h3>Computer Vision</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/788/0*-mZ3aX9UzUmPdWsa"><figcaption>Photo by <a href="https://unsplash.com/@thisisramiro?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Ramiro Mendes</a> on¬†<a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <ol> <li> <strong>Image Processing: </strong>The manipulation and analysis of images to enhance their quality, extract information, or perform specific tasks like noise reduction or edge detection.</li> <li> <strong>Image Segmentation: </strong>The process of dividing an image into multiple segments or regions based on certain criteria, often used in object recognition and scene understanding.</li> <li> <strong>Object Recognition:</strong> The ability to identify and classify objects within an image or video based on their category or¬†type.</li> <li> <strong>Semantic Segmentation:</strong> The task of classifying every pixel in an image into a¬†class.</li> <li> <strong>Instance Segmentation:</strong> Goes beyond semantic segmentation by distinguishing object instances of the same¬†class.</li> <li> <strong>Object Detection: </strong>Identifying and locating objects within an image or video stream, often accompanied by drawing bounding boxes around the detected¬†objects.</li> <li> <strong>Image Classification:</strong> Assigning a single label or class to an entire¬†image.</li> <li> <strong>Feature Extraction:</strong> The process of identifying and extracting distinctive information or features from an image, such as edges, corners, or texture patterns.</li> <li> <strong>Feature Matching:</strong> Comparing extracted features from different images to determine if they correspond to the same object or¬†scene.</li> <li> <strong>Anchor Boxes: </strong>In object detection, anchor boxes are pre-defined bounding boxes of different sizes and aspect ratios that are used to detect objects at various scales and orientations within an¬†image.</li> <li> <strong>Histogram:</strong> A graphical representation of the distribution of pixel values in an image, useful for analyzing image brightness, contrast, and color distribution.</li> <li> <strong>Edge Detection:</strong> The process of identifying abrupt changes in intensity or color in an image, often used as a preprocessing step for feature extraction.</li> <li> <strong>Corner Detection:</strong> Identifying points in an image where the brightness or color changes significantly in multiple directions, commonly used for tracking and recognition.</li> <li> <strong>Hough Transform:</strong> A technique for detecting shapes, such as lines and circles, in an image by transforming them into a parameter space and finding peaks in the transformed space.</li> <li> <strong>Scale-Invariant Feature Transform (SIFT): </strong>A method for detecting and describing local features in images that are invariant to changes in scale, rotation, and illumination.</li> <li> <strong>R-CNN (Region-based Convolutional Neural Network):</strong> R-CNN was one of the early object detection architectures that introduced the concept of region proposals and CNNs for object detection. R-CNN uses selective search to generate region proposals and applies a CNN to each proposal to extract features. It then classifies and refines the proposals. The output of R-CNN includes bounding boxes and class labels for detected¬†objects.</li> <li> <strong>Region Proposals:</strong> In the context of computer vision and object detection, these refer to candidate bounding boxes that represent potential object locations within an image. These region proposals are generated by an algorithm or method before further processing, such as object classification or segmentation. The primary goal of generating region proposals is to reduce the computational cost of exhaustive object detection by focusing only on regions of interest.</li> <li> <strong>Selective Search:</strong> A hierarchical grouping of image regions based on color, texture, and shape similarities.</li> <li> <strong>EdgeBoxes: </strong>An algorithm that identifies bounding boxes around areas with strong edges and¬†corners.</li> <li> <strong>Bounding Box Candidates:</strong> Region proposal methods generate a set of bounding box candidates that are likely to contain objects. These candidates are usually ranked by their likelihood of containing an¬†object.</li> <li> <strong>Non-Maximum Suppression (NMS):</strong> To reduce redundancy in the generated region proposals, non-maximum suppression is often applied. This process eliminates overlapping or highly similar bounding boxes, retaining only the most confident ones.</li> <li> <strong>U-Net:</strong> U-Net is a convolutional neural network architecture designed for semantic segmentation tasks. It is characterized by its U-shaped architecture, with a contracting path to capture context and an expansive path to enable precise localization. U-Net is widely used in medical image segmentation, satellite image analysis, and¬†more.</li> <li> <strong>FCN (Fully Convolutional Network):</strong> FCN is a CNN architecture designed for dense pixel-wise prediction tasks, such as semantic segmentation and image-to-image translation. It replaces fully connected layers with convolutional layers to preserve spatial information.</li> <li> <strong>SegNet:</strong> SegNet is another architecture for semantic segmentation that uses an encoder-decoder structure. It encodes the input image‚Äôs features and then decodes them to produce segmentation maps.</li> <li> <strong>Deeplab:</strong> Deeplab is a family of CNN architectures designed for semantic segmentation, object detection, and image classification. It employs atrous (dilated) convolutions to capture context at different scales and incorporates a ‚Äúcriss-cross‚Äù feature fusion¬†module.</li> <li> <strong>Faster R-CNN (Region-based Convolutional Neural Network):</strong> Faster R-CNN is a popular object detection framework that combines deep learning with region proposal networks (RPNs). It is designed for efficiently and accurately detecting objects in images. Faster R-CNN introduces the RPN, which generates region proposals (candidate bounding boxes) from the input image. These proposals are then used to identify objects using a convolutional neural network¬†(CNN).</li> <li> <strong>Mask R-CNN:</strong> Mask R-CNN is an extension of the Faster R-CNN architecture, designed for instance segmentation tasks. It not only detects objects in an image but also segments them at the pixel level, distinguishing between different instances of the same¬†class.</li> <li> <strong>YOLO (You Only Look Once): </strong>YOLO is a family of object detection architectures known for their real-time performance. YOLO models can detect and classify objects in an image with a single forward pass, making them suitable for real-time applications. YOLO divides the input image into a grid and predicts bounding boxes and class probabilities for each grid cell. This approach allows YOLO to make predictions efficiently.</li> <li> <strong>SSD (Single Shot MultiBox Detector):</strong> SSD is another real-time object detection architecture that combines multiple feature maps from different layers of a CNN to detect objects of various sizes and aspect ratios in a single¬†pass.</li> <li> <strong>PSPNet (Pyramid Scene Parsing Network):</strong> PSPNet is designed for scene parsing and pixel-level labeling. It employs a pyramid pooling module to capture context at different scales and provides fine-grained segmentation.</li> <li> <strong>Pix2Pix:</strong> Pix2Pix is a conditional generative adversarial network (cGAN) architecture for image-to-image translation tasks. It can be used for tasks such as image colorization, style transfer, and edge-to-photo conversion.</li> <li> <strong>CycleGAN:</strong> CycleGAN is a variant of GANs designed for unsupervised image-to-image translation. It learns mappings between two domains without paired training data, making it useful for tasks like style transfer and domain adaptation.</li> <li> <strong>SiamFC (Siamese Fully Convolutional Network):</strong> SiamFC is an architecture used for object-tracking tasks. It learns to track a target object in a video sequence by comparing the similarity between the target‚Äôs appearance and candidates in subsequent frames.</li> </ol> <h3>Frameworks &amp;¬†Tools</h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*rjyYU6x9CuTgXQ2_"><figcaption>Photo by <a href="https://unsplash.com/@brechtcorbeel?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Brecht Corbeel</a> on¬†<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <ol> <li> <strong>TensorFlow: </strong>An open-source machine learning framework developed by¬†Google.</li> <li> <strong>PyTorch:</strong> An open-source machine learning framework known for its flexibility and dynamic computation graph, developed by Facebook‚Äôs AI Research lab¬†(FAIR).</li> <li> <strong>Keras:</strong> A high-level deep learning API that runs on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit (CNTK). It simplifies the process of building and training neural networks.</li> <li> <strong>Theano:</strong> An open-source numerical computation library that specializes in optimizing and evaluating mathematical expressions, often used as a backend for other deep learning frameworks.</li> <li> <strong>Caffe:</strong> A deep learning framework developed by the Berkeley Vision and Learning Center (BVLC). It is known for its speed and efficiency, particularly for image classification tasks.</li> <li> <strong>MXNet:</strong> An open-source deep learning framework developed by Apache. It is designed for both flexibility and efficiency, with support for both symbolic and imperative programming.</li> <li> <strong>CNTK (Microsoft Cognitive Toolkit): </strong>A deep learning framework developed by Microsoft that emphasizes scalability, performance, and production deployment.</li> <li> <strong>PyTorch Lightning:</strong> A lightweight wrapper around PyTorch that simplifies the training loop and other boilerplate code.</li> <li> <strong>OpenCV: </strong>An open-source computer vision library that provides tools for image and video processing, making it valuable for computer vision and image-based machine learning projects.</li> <li> <strong>Pycocotools:</strong> A Python library for evaluating object detection tasks, especially when working with the MS COCO dataset¬†format.</li> <li> <strong>TensorBoard:</strong> A visualization tool provided by TensorFlow for monitoring and debugging machine learning models. It helps visualize metrics, model architectures, and training progress.</li> <li> <strong>Jupyter Notebook:</strong> An interactive web-based environment for creating and sharing documents that contain live code, equations, visualizations, and narrative text. It is widely used for data analysis and machine learning experiments.</li> <li> <strong>PyCharm: </strong>An integrated development environment (IDE) for Python that provides tools for coding, debugging, and profiling, making it suitable for machine learning development.</li> <li> <strong>Anaconda:</strong> A distribution of Python and R for scientific computing and data science. It includes package management and virtual environment capabilities, making it easy to set up and manage machine learning environments.</li> <li> <strong>Docker:</strong> A platform for containerization that allows you to package applications and their dependencies into lightweight containers, simplifying deployment and reproducibility in machine learning projects.</li> <li> <strong>Git:</strong> A distributed version control system used for tracking changes in code repositories. It is essential for collaborative machine learning projects and code versioning.</li> </ol> <figure><img alt="" src="https://cdn-images-1.medium.com/max/788/0*K4zbChbP6XnC_jae"><figcaption>Photo by <a href="https://unsplash.com/@sunnystate?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Reuben Juarez</a> on¬†<a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="external nofollow noopener" target="_blank">Unsplash</a></figcaption></figure> <p>In this quick revision of deep learning terminologies, we‚Äôve covered fundamental concepts essential for navigating the world of artificial intelligence. From neural network architectures like CNNs and LSTMs to training procedures, regularization techniques, and evaluation metrics, these terms provide the building blocks for understanding and working with deep learning models. With knowledge of frameworks like TensorFlow and PyTorch and tools like Jupyter Notebook, data scientists and researchers can explore and innovate in this rapidly evolving¬†field.</p> <p><strong>Stay tuned for more upcoming articles by following me and keeping updated! Your readership is greatly appreciated.</strong></p> <p><strong>Thank you for taking the time to read this article!¬†</strong>üì¢üòä</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7e9d88e1352f" width="1" height="1" alt=""></p> </body></html>