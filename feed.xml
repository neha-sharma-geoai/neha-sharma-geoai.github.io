<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://neha-sharma-geoai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://neha-sharma-geoai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-12T09:11:07+00:00</updated><id>https://neha-sharma-geoai.github.io/feed.xml</id><title type="html">blank</title><subtitle># A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Smart Buildings and Cities with Remote Sensing and GIS: First-of-Its-Kind Global Volume Bridging…</title><link href="https://neha-sharma-geoai.github.io/blog/2025/smart-buildings-and-cities-with-remote-sensing-and-gis-first-of-its-kind-global-volume-bridging/" rel="alternate" type="text/html" title="Smart Buildings and Cities with Remote Sensing and GIS: First-of-Its-Kind Global Volume Bridging…"/><published>2025-06-08T14:48:19+00:00</published><updated>2025-06-08T14:48:19+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2025/smart-buildings-and-cities-with-remote-sensing-and-gis-first-of-its-kind-global-volume-bridging</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2025/smart-buildings-and-cities-with-remote-sensing-and-gis-first-of-its-kind-global-volume-bridging/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Edited by: Dr. Shiv Mohan, Dr. Navneet Munoth, Ar. Neha Sharma Edition: 1st Edition, 2025 Publisher: Taylor &amp; Francis Publication Location&#x2026;Continue reading on Medium »]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://neha-sharma-geoai.github.io/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2025/plotly</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://neha-sharma-geoai.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">What’s special about ISRO’s XPoSat?</title><link href="https://neha-sharma-geoai.github.io/blog/2024/whats-special-about-isros-xposat/" rel="alternate" type="text/html" title="What’s special about ISRO’s XPoSat?"/><published>2024-07-08T21:42:47+00:00</published><updated>2024-07-08T21:42:47+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2024/whats-special-about-isros-xposat</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2024/whats-special-about-isros-xposat/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[On January 1, 2024, ISRO&#x2019;s PSLV-C58 successfully launched XPoSat, India&#x2019;s first dedicated satellite for X-ray polarimetry. &#x1F680; The&#x2026;Continue reading on Medium »]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://neha-sharma-geoai.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://neha-sharma-geoai.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2024/tabs</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="507fac3b-a858-4939-b9d9-0a8b1b631624" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="507fac3b-a858-4939-b9d9-0a8b1b631624" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="28d8f3f6-1dfd-4f68-b9a6-ff117513b75d" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="28d8f3f6-1dfd-4f68-b9a6-ff117513b75d" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="df38ec81-c89b-4025-9ae8-81ae1666f65b" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="df38ec81-c89b-4025-9ae8-81ae1666f65b" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://neha-sharma-geoai.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2024/typograms</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://neha-sharma-geoai.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://neha-sharma-geoai.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">Parting Ways from Unimodal Single-Tasking to Multimodal Multitasking Deep Learning Models</title><link href="https://neha-sharma-geoai.github.io/blog/2024/parting-ways-from-unimodal-single-tasking-to-multimodal-multitasking-deep-learning-models/" rel="alternate" type="text/html" title="Parting Ways from Unimodal Single-Tasking to Multimodal Multitasking Deep Learning Models"/><published>2024-02-18T08:27:03+00:00</published><updated>2024-02-18T08:27:03+00:00</updated><id>https://neha-sharma-geoai.github.io/blog/2024/parting-ways-from-unimodal-single-tasking-to-multimodal-multitasking-deep-learning-models</id><content type="html" xml:base="https://neha-sharma-geoai.github.io/blog/2024/parting-ways-from-unimodal-single-tasking-to-multimodal-multitasking-deep-learning-models/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QXN-VTxj0yP1J8Zp"/><figcaption>Photo by <a href="https://unsplash.com/@norevisions?utm_source=medium&amp;utm_medium=referral">No Revisions</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>Transitioning from single-tasking to multi-tasking deep learning models opens exciting possibilities for solving complex problems in the real world.</p> <p>Multitasking models, also known as Multitask Learning (MTL) models, are a type of deep learning architecture designed to tackle multiple related tasks simultaneously. This differs from traditional single-task models, which focus on learning and performing just one specific task.</p> <p>Multimodal multitasking models take the already powerful concept of <strong>multitasking models</strong> (MTL) a step further. They introduce the additional complexity of dealing with <strong>multiple data modalities</strong> concurrently. This means instead of learning from just one type of data (e.g., images), they can process and integrate information from various sources like <strong>RGB, text, LiDAR, hyperspectral, SAR data</strong>, and others, performing multiple tasks simultaneously.</p> <p><strong>Key points about multimodal multitasking models:</strong></p> <ul><li><strong>Combined Data Power:</strong> By fusing information from diverse sources, these models capture a richer and more comprehensive understanding of the problem at hand. Geospatial data comes from a variety of sources. Imagine analyzing a building not just from its image but also with height information from LiDAR and text descriptions, leading to a more accurate understanding.</li><li><strong>Enhanced Task Performance:</strong> The synergistic learning between modalities can improve the performance of individual tasks compared to traditional single-task or single-modality approaches.</li><li><strong>Real-World Applicability:</strong> Analogous to our brain processing surrounding objects collectively, deep learning models, inspired by the human brain, are tailored for real-world scenarios where information arrives in diverse forms. This type of model is particularly suited for real-world scenarios where information comes in different forms. For example, understanding urban landscapes, analyzing medical images with patient notes, or interpreting autonomous vehicle sensor data all benefit from this multimodal approach.</li></ul> <h3><strong>Applications in the Geospatial Domain</strong></h3> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*E44PhnP4B07Nt0Eq"/><figcaption>Photo by <a href="https://unsplash.com/@starworshipp3r?utm_source=medium&amp;utm_medium=referral">June</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <ul><li><strong>Joint building segmentation and height estimation:</strong> Combining imagery (RGB, SAR) and LiDAR data in an MMTL model allows for simultaneous extraction of building outlines and estimation of their heights. This information can be valuable for urban planning, disaster response, and other applications.</li></ul> <p>Imagine having a tool that automatically analyzes large-scale satellite and aerial imagery, extracting detailed information about every building in a city—its precise location, shape, and even height—all from a single analysis. This isn’t science fiction anymore; it’s the potential of Multimodal Multitasking Learning (MMTL) in urban analysis, exemplified by the joint task of building segmentation and height estimation.</p> <p><strong>How does MMTL work?</strong></p> <p>Traditional approaches often treat these tasks separately, leading to inefficiencies and potential inaccuracies. MMTL breaks this barrier by:</p> <ol><li><strong>Leveraging multiple data sources:</strong> It combines imagery (RGB, SAR, or hyperspectral) with LiDAR data or DEM data, offering both visual and 3D information.</li><li><strong>Performing multiple tasks simultaneously:</strong> It segments individual buildings in the imagery while simultaneously estimating their heights.</li><li><strong>Learning shared features:</strong> The model learns from both tasks simultaneously.</li></ol> <p>For example, after a natural disaster, MMTL can quickly assess building damage by accurately segmenting and measuring damaged structures. This information helps prioritize rescue efforts, allocate resources efficiently, and inform reconstruction plans.</p> <p>MMTL can create 3D city models with precise building heights and footprints, enabling simulations of traffic flow, energy consumption, and air quality. Predicting how airflows and temperatures are affected by building shapes and heights informs urban design decisions that promote comfort and sustainability.</p> <p>MMTL can automatically estimate building sizes and values, leading to fairer and more efficient property tax assessments. This reduces administrative burden and ensures accurate revenue generation for city development.</p> <p>MMTL can optimize the placement of green spaces based on building heights and access to sunlight.</p> <p>MMTL can identify vulnerable populations residing in buildings with poor ventilation or high solar heat gain.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*GMmfnQSNVfatyIfq"/><figcaption>Photo by <a href="https://unsplash.com/@chrumo?utm_source=medium&amp;utm_medium=referral">Piotr Chrobot</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <ul><li><strong>Land cover classification and change detection:</strong> Leverage imagery (multispectral, hyperspectral), LiDAR, and elevation data within an MMTL model to simultaneously classify land cover types and detect changes over time, providing valuable insights for environmental monitoring and resource management.</li></ul> <p>MMTL can analyze building density and land use patterns, guiding developers in identifying suitable locations for new construction projects while considering urban needs and growth patterns.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kJBTK3VaS7OUEv-9"/><figcaption>Photo by <a href="https://unsplash.com/@usgs?utm_source=medium&amp;utm_medium=referral">USGS</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <ul><li><strong>Autonomous vehicle navigation:</strong> MMTL combines LiDAR for 3D environment perception, a camera for visual understanding, GPS for location, and sensor data for real-time updates. Integrating tasks like object detection, lane recognition, and traffic sign identification into a single model could contribute to safer and more efficient self-driving cars. Combining data from different modalities allows for a richer understanding of the environment, capturing both visual cues and precise 3D information. The model can learn and adapt to diverse driving conditions like rain, snow, or low light based on the combined sensor data.</li></ul> <p><strong>Example:</strong> Imagine an AV approaching an intersection.</p> <ul><li><strong>Camera:</strong> Sees a car approaching but struggles to judge its distance due to poor lighting.</li><li><strong>LiDAR:</strong> Provides precise distance measurement, confirming the potential collision.</li><li><strong>GPS:</strong> Tracks the AV’s position and helps plan a safe maneuver.</li><li><strong>IMU:</strong> Detects the AV’s orientation and potential instability.</li></ul> <p>MMTL combines this information in real-time, allowing the AV to:</p> <ul><li><strong>Accurately assess the collision risk.</strong></li><li><strong>Plan a safe maneuver (braking, swerving, etc.).</strong></li><li><strong>Remain stable throughout the maneuver.</strong></li></ul> <p><strong>Tasks:</strong></p> <ul><li><strong>Object Detection:</strong> Identifying and classifying various objects on the road (pedestrians, vehicles, traffic signs, etc.).</li><li><strong>Lane Detection:</strong> Recognizing and staying within designated lanes for proper navigation.</li><li><strong>Road Condition Estimation:</strong> Understanding the road surface (wet, uneven, etc.) for safe maneuverability.</li><li><strong>Traffic flow prediction:</strong> Anticipating the behavior of other vehicles and pedestrians.</li><li><strong>Trajectory Planning:</strong> Determining the optimal path the vehicle should take while obeying traffic rules and avoiding obstacles.</li></ul> <p><strong>Real-world Examples:</strong></p> <ul><li>Tesla’s Autopilot system utilizes MMTL to combine camera, radar, and other data for improved object detection and obstacle avoidance.</li><li>Waymo’s self-driving cars use MMTL to fuse camera, LiDAR, and radar data for safe navigation in complex urban environments.</li></ul> <p><strong>Challenges and Future:</strong></p> <ul><li><strong>Data Quality and Availability:</strong> Training MMTL models requires large datasets with diverse driving scenarios, which can be expensive and challenging to collect.</li><li><strong>Computational Complexity:</strong> MMTL models can be computationally demanding, requiring powerful hardware and efficient algorithms for real-time operation in AVs.</li><li><strong>Explainability and Trust:</strong> Understanding how the model makes decisions is crucial for building trust in AV technology.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*lBet8Tq9wlu2F0Tu"/><figcaption>Photo by <a href="https://unsplash.com/@isthatbrock?utm_source=medium&amp;utm_medium=referral">Brock Wegner</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <ul><li><strong>Infrastructure Monitoring and Maintenance: </strong>Utilize imagery, LiDAR, and sensor data from infrastructure like bridges, pipelines, and power grids within an MMTL model to simultaneously detect potential defects, estimate damage, and predict maintenance needs. This leads to increased safety, reduced costs, and improved resilience. MMTL can analyze diverse data sources simultaneously, such as:</li></ul> <p><strong>Visual data:</strong> images from drones, satellite imagery, CCTV footage.</p> <p><strong>Sensor data:</strong> data from strain gauges, vibration sensors, and temperature sensors.</p> <p><strong>Historical data:</strong> Maintenance records, inspection reports, weather data.</p> <p>This approach helps to:</p> <ul><li><strong>Identify potential issues early:</strong> MMTL can detect subtle changes in data patterns that might indicate developing problems before they become critical.</li><li><strong>Prioritize maintenance activities:</strong> The model can assess the severity of identified issues and recommend which infrastructure components need immediate attention.</li><li><strong>Optimize maintenance schedules:</strong> MMTL can predict the lifespan of various infrastructure components and suggest preventive maintenance actions to avoid unexpected failures.</li><li><strong>Reduce costs:</strong> By identifying and addressing issues early, MMTL can prevent costly repairs and disruptions.</li></ul> <p>Here are some <strong>real-world use cases</strong> demonstrating the power of MMTL in infrastructure monitoring and maintenance:</p> <p><strong>1. Bridge Inspection:</strong></p> <ul><li><strong>Use case:</strong> Combine drone images with sensor data (vibrations, cracks) to identify structural weaknesses in bridges.</li><li><strong>Tasks:</strong> Crack detection, corrosion analysis, structural integrity assessment.</li><li><strong>Benefits:</strong> Reduce the need for manual inspections, detect hidden issues, prioritize repairs, and extend bridge lifespan.</li></ul> <p><strong>2. Road Maintenance:</strong></p> <ul><li><strong>Use case:</strong> Analyze satellite imagery, LiDAR data, and traffic flow data to detect road damage like potholes or cracks.</li><li><strong>Tasks:</strong> Pothole detection, crack analysis, and road surface condition assessment.</li><li><strong>Benefits:</strong> Improve road safety, optimize maintenance scheduling, and reduce repair costs.</li></ul> <p><strong>3. Pipeline Monitoring:</strong></p> <ul><li><strong>Use case:</strong> Analyze sensor data (pressure, temperature, flow) and satellite imagery to detect leaks or corrosion in pipelines.</li><li><strong>Tasks:</strong> Leak detection, corrosion analysis, pipeline integrity assessment.</li><li><strong>Benefits:</strong> Prevent environmental damage, ensure pipeline safety, and optimize maintenance schedules.</li></ul> <p><strong>4. Rail Track Inspection:</strong></p> <ul><li><strong>Data sources:</strong> Cameras, LiDAR, vibration sensors.</li><li><strong>Tasks:</strong> Track defect detection, wear and tear analysis, track stability assessment.</li><li><strong>Benefits:</strong> MMTL can identify track defects like misalignment, broken rails, and loose fasteners before they cause accidents or derailments. This ensures the safety and efficiency of railway operations.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*7fvEoK69XkF3sjUm"/><figcaption>Photo by <a href="https://unsplash.com/@suntooth?utm_source=medium&amp;utm_medium=referral">Suntooth</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <ul><li><strong>Flood Risk Assessment and Mapping: </strong>Combine satellite imagery, LiDAR data, and precipitation information in an MMTL model to simultaneously predict flood inundation areas, estimate flood depths, and assess potential damage.</li></ul> <p><strong>Scenario:</strong> Imagine a coastal city like Miami, Florida, prone to both riverine flooding from heavy rains and storm surge flooding due to hurricanes. The city authorities want to conduct a comprehensive flood risk assessment to:</p> <ul><li><strong>Identify flood-prone areas:</strong> Accurately map areas at risk of flooding from various sources, including rivers, storm surges, and drainage overflows.</li><li><strong>Assess vulnerability:</strong> Analyze the vulnerability of different infrastructures, populations, and economic assets based on factors like elevation, building type, and socio-economic status.</li><li><strong>Predict flood inundation:</strong> Estimate the extent and depth of potential floodwater in different scenarios (e.g., 100-year flood, major hurricane).</li><li><strong>Inform mitigation strategies:</strong> Develop targeted and effective mitigation strategies based on the identified risks and vulnerabilities.</li></ul> <p><strong>How MMTL can be used:</strong></p> <ol><li><strong>Data Sources:</strong></li></ol> <ul><li><strong>Satellite imagery:</strong> High-resolution optical, radar, and LiDAR data for land cover classification, elevation mapping, and coastal features identification.</li><li><strong>Precipitation data:</strong> Rainfall gauges and weather models for predicting flood events from rivers.</li><li><strong>Storm surge data:</strong> Hurricane track forecasts and historical storm data for simulating storm surge impacts.</li><li><strong>Social media data:</strong> Tweets and images for real-time updates on flood events and community needs.</li><li><strong>Hydrological data:</strong> River gauge data and historical flood records for understanding riverine flood patterns.</li><li><strong>City infrastructure data:</strong> Building footprint data, population density information, and critical infrastructure locations.</li></ul> <p><strong>2. Tasks:</strong></p> <ul><li><strong>Flood hazard mapping:</strong> Using LiDAR and satellite imagery, identify low-lying areas prone to flooding from rivers and storm surges.</li><li><strong>Vulnerability assessment:</strong> Combine social media data, population density, and building-type information to identify vulnerable communities and critical infrastructure.</li><li><strong>Flood inundation modeling:</strong> Use MMTL to combine river flow data, storm surge models, and elevation data to predict floodwater extent and depth in different scenarios.</li><li><strong>Risk communication:</strong> Generate interactive maps and reports to communicate flood risks to stakeholders and communities.</li></ul> <p><strong>3. Benefits:</strong></p> <ul><li><strong>Improved accuracy:</strong> MMTL can integrate diverse data sources, leading to more accurate flood risk assessments compared to traditional methods relying on limited data.</li><li><strong>Enhanced understanding:</strong> MMTL can identify complex relationships between different factors like elevation, land cover, and infrastructure, leading to better mitigation strategies.</li><li><strong>Real-time insights:</strong> Social media data allows for real-time updates on flood events, enabling faster response and targeted mitigation efforts.</li><li><strong>Data efficiency:</strong> MMTL can learn from limited data, making it suitable for coastal areas with sparse historical flood records.</li></ul> <p><strong>4. Mitigation Strategies:</strong></p> <ul><li>Based on the MMTL analysis, the city can prioritize building seawalls and levees in high-risk coastal areas.</li><li>Invest in green infrastructure like parks and rain gardens to improve drainage and reduce floodwater inundation.</li><li>Implement early warning systems and evacuation plans for vulnerable communities.</li><li>Develop flood-resistant building codes and retrofit critical infrastructure.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*sJtIQsCI3H9TBhVl"/><figcaption>Photo by <a href="https://unsplash.com/@noaa?utm_source=medium&amp;utm_medium=referral">NOAA</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <ul><li><strong>Urban Sprawl Analysis and Infrastructure Planning: </strong>MMTL analyzes spatial patterns in imagery and LiDAR, combined with demographic information, to understand urban sprawl patterns and predict future growth. This helps in planning infrastructure like roads, utilities, and green spaces more efficiently and sustainably.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*viRucMEayuUpTI_F"/><figcaption>Photo by <a href="https://unsplash.com/@scottwebb?utm_source=medium&amp;utm_medium=referral">Scott Webb</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <ul><li><strong>Crop Yield Prediction and Monitoring:</strong></li></ul> <p><strong>Data inputs:</strong> Satellite imagery (multispectral, hyperspectral), weather data, soil moisture sensors, and historical yield data.</p> <p><strong>Tasks:</strong> Predict crop yields, identify areas with potential crop stress or disease, and monitor crop growth stages.</p> <p><strong>Benefits:</strong> Informs farmers about potential risks and allows for targeted interventions to optimize yields and resource use.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*uvcgA6w6xUxOMh9B"/><figcaption>Photo by <a href="https://unsplash.com/@opeleye?utm_source=medium&amp;utm_medium=referral">Jordan Opel</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <h3>Implementation of MMTL Models:</h3> <p>Here’s a breakdown of how Multimodal Multitasking Learning (MMTL) works:</p> <p><strong>Data Inputs:</strong></p> <ul><li><strong>Multiple data sources:</strong> Each block represents a different data source, like satellite/drone imagery, text, LiDAR data, etc. (e.g., Block A: Sensor images, Block B: LiDAR data, Block C: Social media text).</li><li><strong>Preprocessing:</strong> Each data source might undergo specific preprocessing depending on its type (e.g., image resizing, text tokenization, normalization, scaling etc).</li></ul> <p><strong>Feature Extraction:</strong></p> <ul><li><strong>Individual encoders:</strong> Each data source has its own encoder network specifically designed to extract relevant features (e.g., Convolutional Neural Network (CNN) for images, Recurrent Neural Network (RNN) for text).</li><li><strong>Shared representations:</strong> Extracted features from different encoders are combined through a fusion layer, creating a shared representation capturing information from all modalities.</li></ul> <p><strong>Multitasking:</strong></p> <ul><li><strong>Multiple task heads:</strong> Different heads are connected to the shared representation, each specializing in a specific task (e.g., object detection, building segmentation, road condition estimation).</li><li><strong>Individual outputs:</strong> Each task head produces its own output based on the shared representation and its specific training data.</li></ul> <p><strong>Overall Training:</strong></p> <ul><li><strong>Combined Loss:</strong> A single loss function combines individual task losses, encouraging the model to learn features beneficial for all tasks.</li><li><strong>Backpropagation:</strong> Error signals from each task loss and the combined loss are used to update the parameters of all encoders, the fusion layer, and the task heads simultaneously.</li></ul> <p><strong>Challenges:</strong></p> <ul><li><strong>Model Complexity:</strong> Designing and training MMTL models can be more complex compared to single-modality or single-task models.</li><li><strong>Data Quality and Availability:</strong> Requires high-quality, accurately labeled data from all modalities and tasks.</li><li><strong>Task Interference:</strong> Balancing different tasks and mitigating potential negative influence between them is crucial.</li></ul> <p><strong>Remember:</strong> This is a simplified representation, and actual MMTL architectures can vary depending on the specific application and data types.</p> <p>Designing and training these models requires advanced architectures and specialized training techniques, making them more computationally expensive compared to simpler models. Collecting and preparing data from multiple modalities can be challenging and require careful curation and preprocessing. Choosing tasks that are compatible and can be effectively learned from the available data modalities is crucial for successful implementation.</p> <p><strong>Overall, MMTL holds exciting potential to revolutionize the geospatial domain by:</strong></p> <ul><li><strong>Extracting deeper insights from multiple data sources.</strong></li><li><strong>Solving complex problems with improved efficiency and accuracy.</strong></li><li><strong>Enabling proactive and data-driven decision-making.</strong></li></ul> <p>As MMTL technology continues its evolution, we anticipate a plethora of innovative applications contributing to a more sustainable, resilient, and data-driven future for our planet.</p> <p>Feel free to share your views or additional examples of MMTL in geospatial analysis in the <strong>comments section</strong>.</p> <p>In our next article, we’ll delve into <strong>Python libraries</strong> supporting the development of multimodal multitask learning.</p> <p>Don’t forget to follow up for updates on forthcoming geospatial articles! 🔔</p> <p>Thank you for reading this article. 😊</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ea81b2a77b55" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>